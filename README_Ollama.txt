Ollama Integration Guide

This project includes components that utilize Ollama for running large
language models locally. Ollama helps in: - Running LLaMA and other
models offline - Fast inference with GPU/CPU acceleration - Easy model
management using ollama run, ollama pull, and custom model files

Key Features Used:

-   Local inference for privacy-focused AI
-   Integration with Python backend
-   Embedding generation for vector search

Notes:

Ensure Ollama is installed: https://ollama.com/download

Run a model: ollama run llama3
